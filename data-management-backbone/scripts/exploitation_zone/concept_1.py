# -*- coding: utf-8 -*-
"""concept_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XOZ5fyw7ksS-k1oHJOrxFIYCmcJoucer
"""

import duckdb
import pandas as pd
import numpy as np
import os
import re

from pathlib import Path

LACIUDADALDIA_TRUSTED_DIR = "trusted/laciudadaldia.parquet"
ARTICLES_TRUSTED_DIR = "trusted/articles/articles.csv"

EXPLOITATION_DIR = "exploitation/"
CONCEPT1_FILE = "concept1.parquet"

con = duckdb.connect()

laciudadaldia = pd.read_parquet(LACIUDADALDIA_TRUSTED_DIR)
articles = pd.read_csv(ARTICLES_TRUSTED_DIR)

"""After loading both dataframes, we merge them based on the date (with one entry per day) and save the result as a parquet file in the exploitation zone.

The primary objective is to delve into the datasets focusing on two key themes:

1. Metrics of "Barcelona al dia" in each article, exploring how a particular article might influence Barcelona's economy.
2. The significance and influence of articles on a daily basis. We collate statistics from all articles for each day and analyze them alongside the Barcelona metrics.
"""

# Concept 1
df1 = pd.merge(articles, laciudadaldia, left_on='SQLDATE', right_on='data_indicador', how='left')

df1.drop('data_indicador', axis=1, inplace=True)

# Guardar en .parquet
df1.to_parquet(os.path.join(EXPLOITATION_DIR, CONCEPT1_FILE))